\begin{mysection}{Hash Tables}

Many applications require the use of an abstract data type called a \concept{dictionary}, which maintains a dynamic set of items, each with a distinct key. The operations supported by this dictionary are \procedure{Insert}, \procedure{Delete} and \procedure{Search} (search for the item in the dictionary with the given key, if such item exists). These data structures are typically implemented using a very powerful data structure called \concept{hash tables}, which allows to perform the mentioned operations in $\Theta(1)$ time. 

    A part from dictionaries, hash tables and their fundamental hash functions, have plenty of applications: databases, compiler variable definitions, network router lookup tables, string matching (Chapter 32), cryptographic hash functions ...

\subsection{Direct-access tables}
Suppose we need a dynamic set where each element has an integer key drawn from the universe $\mathcal{U} = \{0, 1, \dotsc, m - 1 \}$. A simple and primitive approach is to use direct-access tables, which stores each element in an array indexed by the key of the element.

\begin{center}
    \tikzstyle{mymat}=[
      matrix of nodes, %math nodes,
      text height=7pt,
      text depth=2pt,
      text width=36pt,
      % align=center,
      % anchor=west,
      column sep=-\pgflinewidth,
      nodes={draw},
      column 1/.style={align=center},
      column 2/.style={nodes={draw=none}, align=left, font={\footnotesize}, text width=30pt}
    ]
    \tikzstyle{nil}=[fill=black!14]
\begin{tikzpicture}
% \matrix[matrix of nodes,
        % nodes={draw, minimum size=18pt, anchor=center},
        % nodes in empty cells, 
        % minimum height = 8pt,
        % column 2/.style={nodes={draw=none}, font={\small}, align=left}] 
    % (m) at (0, 0)
    \matrix[mymat] 
        (table) at (5, -1.7)
    {
      | [nil] |\textit{/}     & 0       \\
      $\textrm{item}_1$       & 1       \\
      | [nil] |\textit{/}     & 2       \\
      $\textrm{item}_3$       & 3       \\
      | [nil] |\textit{/}     &         \\
      | [nil] |\textit{/}     &         \\
      $\textrm{item}_{m - 2}$ & $m - 2$ \\
      | [nil] |\textit{/}     & $m - 1$ \\
    };

    \draw[fill=black!14] (-0.2, -1.7) circle [x radius=65pt,y radius=60pt] ;
    \draw[fill=white] (-0.2, -2.6) circle [x radius=50pt,y radius=30pt] ;

    \node (U) at (-0.2,0)  {$\mathcal{U}$};
    \node     at (-0.2,-0.3) [font={\footnotesize}] {(universe of keys)};
    \node  at (-0.4,-1)  {0};
    \node  at (-1.1,-0.8) {2};
    \node  at (-1.8,-1.3) {5};
    \node  at (0.8,-0.9) {$m - 1$};


    \node (K) at (-0.9,-2.4) {$\mathcal{K}$};
    \node     at (-0.9,-2.7) [font={\footnotesize}] {(actual keys)};
    \node (1) at (0.2,-2.0) {1};
    \draw[->] (1) -- (table-2-1.west);
    \node (3) at (0.9,-2.4) {3};
    \draw[->] (3) -- (table-4-1.west);
    \node (m-2) at (0.6,-3.0) {$m - 2$};
    \draw[->] (m-2) -- (table-7-1.west);

\end{tikzpicture}
\end{center}


\noindent Where the actual keys $\mathcal{K}$, are the actual keys being used by the dictionary. This approach has two big drawbacks:
\begin{smallenumerate}
    \item Keys may not be integers.
    \item Big memory usage to keep array for ALL possible key values.
\end{smallenumerate}

The first problem is solved with \concept{prehashing}: all keys are finite and discrete, thus can be represented as a finite string of bits.
 The second problem will be solved with another approach in next section.


\subsection{Hash Tables}
When the set $\mathcal{K}$ of keys stored in adictionary is much smaller than the universe $\mathcal{U}$ of all possible keys, a hash table requires much less storage than a direct address table. Specifically, we can reduce the storage requirement to $\Theta(\absolute{\mathcal{K}})$, while maintaining the $\Theta(1)$ time for search operations.

With hashing we define a \concept{hash function} 
\[
    h: \mathcal{U} \to \{0, 1, \dotsc, m - 1\},
\]
which maps any key from the universe of keys, to a slot in a \concept{hash table} $T[0\,..\,m - 1]$. There is one hitch: since typically $\absolute{\mathcal{U}} > m$, by the pigeon-hole principle, there will different keys which will be mapped to the same slot, a situation called a \concept{collision}.

Of course, the ideal situation is to have the least collisions at all, which will be achieved by a propper hash function $h$. We present now a method for resolving collisions called chaining. Later chapters present an alternative collisino resolution method called open addressing.

\subsubsection{Chaining}
In \concept{chaining} we place all the elements that hash to the same slot, into the same linked list. We store also the key and the value in the list so that we can searh for a given key in each slot, and the list can be doubly-linked for ease of deletion of items.

Thus the dictionary operations on a hash table $T$ are easy to implement with chaining:

\begin{pseudocode}{Chained-Hash-Insert}{T, x}
    \IF {there is no element with $x.key$ in list $T[h(x.key)]$}
        \STATE insert $x$ at the head of the list
    \ELSE
        \STATE override such element with the new element $x$
    \ENDIF
\end{pseudocode}
\vspace{2mm}
\begin{pseudocode}{Chained-Hash-Delete}{T, x}
    \STATE delete x from the list $T[h(x.key)]$
\end{pseudocode}
\vspace{2mm}
\begin{pseudocode}{Chained-Hash-Search}{T, k}
    \STATE search for an element with key $k$ in list $T[h(k)]$
\end{pseudocode}


\vspace{5mm}
All these operations require searching in the list $T[h(k)]$ for some key $k$. Thus the worst case time of these operations is proportional to the length of the list $T[h(k)]$. Thus if $n = \absolute{\mathcal{K}}$, the worst-case behavior for hashing with chaining is then all $n$ elements end up in the same linked list, and these operations take $\Theta(n)$ time! Hash tables however, have a very good average case (and empirically tested). We can make a first assumption for ease of analysis (however, this assumption ends up being false in reality).


\begin{center}
    \tikzstyle{mymat}=[
      matrix of nodes, %math nodes,
      text height=7pt,
      text depth=2pt,
      text width=15pt,
      nodes in empty cells, 
      % align=center,
      % anchor=west,
      column sep=-\pgflinewidth,
      nodes={draw}, 
      node distance=60pt,
      column 1/.style={align=center},
      column 2/.style={nodes={draw=none}, align=left, font={\footnotesize}, text width=30pt}
    ]
    \tikzstyle{mylist}=[
      matrix of nodes, %math nodes,
      text height=7pt,
      node distance=50pt,
      text depth=2pt,
      text width=15pt,
      nodes in empty cells, 
      align=center,
      % anchor=west,
      column sep=-\pgflinewidth,
      nodes={draw},
    ]
    \tikzstyle{nil}=[fill=black!14]
    \tikzstyle{arrow}=[->]
\begin{tikzpicture}
% \matrix[matrix of nodes,
        % nodes={draw, minimum size=18pt, anchor=center},
        % nodes in empty cells, 
        % minimum height = 8pt,
        % column 2/.style={nodes={draw=none}, font={\small}, align=left}] 
    % (m) at (0, 0)
    \matrix[mymat] 
        (table) at (3.4, -1.6)
    {
      | [nil] |\textit{/}     & 0       \\
                              &         \\
      | [nil] |\textit{/}     &         \\
      | [nil] |\textit{/}     &         \\
                              &         \\
      | [nil] |\textit{/}     &         \\
      | [nil] |\textit{/}     & $m - 1$ \\
    };

    \matrix[mylist] 
        (l11) at (0,0) [right of=table-2-1]
    {
      $k_7$ & | [nil] |\textit{/} \\ 
    };

    \matrix[mylist] 
        (l21) at (0,0) [right of=table-5-1]
    {
      $k_3$ &   \\ 
    };

    \matrix[mylist] 
        (l22) at (0,0) [right of=l21-1-2]
    {
      $k_n$ & | [nil] |\textit{/} \\ 
    };

    \draw[fill=black!14] (0,-1.6) circle [x radius=50pt,y radius=55pt] ;
    \draw[fill=white] (0, -2) circle [x radius=30pt,y radius=40pt] ;

    \node (U) at (0,0)  {$\mathcal{U}$};
    \node     at (0,-0.3) [font={\footnotesize}] {(universe of keys)};


    \node (K) at (0,-1.0) {$\mathcal{K}$};
    \node     at (0,-1.3) [font={\footnotesize}] {(actual keys)};
    \node (7) at (0.2,-2.0) {$k_7$};
    \draw[arrow] (1) -- (table-2-1.west);
    \node (3) at (-0.3,-2.4) {$k_3$};
    \draw[arrow] (3) -- (table-5-1.west);
    \node (n) at (0.1,-3.0) {$k_n$};
    \draw[arrow] (n) -- (table-5-1.west);

    \draw[arrow] ([xshift=-75pt]l11.east) -- ([xshift=3.6pt]l11.west);
    \draw[arrow] ([xshift=-75pt]l21.east) -- ([xshift=3.6pt]l21.west);
    \draw[arrow] ([xshift=-75pt]l22.east) -- ([xshift=3.6pt]l22.west);

\end{tikzpicture}
\end{center}



\paragraph{Simple Uniform Hashing:} Each key is equally likely to be hashed to any of the $m$ slots in the table, independent of where any key element has hashed to.

\vspace{2mm}
Then, for $j = 0, 1, \dotsc, m - 1$, let us denote the length of the list T[j] by $n_j$, so that $n = n_0 + n_1 + \dots + n_{m - 1}$. Hence, the expected value of $n_j$ is $\expectation{n_j} = n/m  \equiv \alpha$, called the \concept{load factor} (note that $\alpha = \alpha(n)$, is a function of $n$). Assuming $O(1)$ time to compute the hash function, the time required to search for an element with key $k$ depends linearly o the length $n_{h(k)}$ of the list $T[h(k)]$. So, lets study the expected number of elements examined by the search algorithm in the list $T[h(k)]$ to find key $k$. We have two cases, successful and unsuccessful search. 

\begin{theorem}
    In a hash table in which collisions are resolved by chaining, an unsuccessful search takes average-case run time $\Theta(1 + \alpha)$, under the assumption of simple uniform hashing.
\end{theorem}
\begin{proof}
    Under the assumption of simple uniform hashing, any key $k$ not already in the table is equally likely to hash to any of the $m$ slots. The expected time to unsuccessfuly search for key $k$ is the expected time to search at the end of the list $T[h(k)]$, which has expected length $\expectation{n_{h(k)}} = \alpha$. Thus, the expected number of elements examined in an unsuccessful search is \alpha, and the total time required (including computation of $h(k)$) is $\Theta(1 + \alpha)$.
\end{proof}

\begin{theorem}
    In a hash table in which collisions are resolved by chaining, a successful search takes average-case run time $\Theta(1 + \alpha)$, under the assumption of simple uniform hashing.
\end{theorem}
\begin{proof}
    For successful search we have a different situation, since each list is not equally likely to be searched. Insetead, the probability that a list is searched is proportional to the number of elements it contains.

    We assume that the element being tsearched for is equally likely to be any of the $n$ elements stored in the table. The number of elements examined during a successful search for element $x$ is one more than the number of elements that appear before $x$ in $x$'s list. Since new elements are placed at the front of the list, elements before $x$ in the list were all inserted after $x$ was inserted. 

    To find the expected number of elements examined, take average over $n$ elements of $1$ plus the exepcted number of elements added to $x$'s list after $x$ was added to the list. Let $x_i$ denote the $i$th element inserted into the table for $i = 1, 2, \dotsc, n$; and let $k_i = x_i.key$. For keys $k_i$ and $k_j$, define the indicador random variable $X_{ij} = \mathbbm{1}\{h(k_i) = h(k_j)\}$. Under the assumption of simple uniform hashing, $\prob{h(k_i) = h(k_j)} = 1/m$, thus $\expectation{X_{ij}} = 1/m$. Hence the expected number of elements examined in a successful search is:
\begin{align*}
    \expectation{\frac{1}{n} \sum_{i = 1}^{n} \left(1 + \sum_{j = i + 1}^{n}X_{ij} \right) } 
    &= \frac{1}{n} \sum_{i = 1}^{n} \left(1 + \sum_{j = i + 1}^{n}\expectation{X_{ij}} \right)  \\
    &= \frac{1}{n} \sum_{i = 1}^{n} \left(1 + \sum_{j = i + 1}^{n}\frac{1}{m}\right)  \\
    &= 1 + \frac{n - 1}{2m} \\
    &= 1 + \frac{\alpha}{2} - \frac{\alpha}{2m}.
\end{align*}
    Therefore, total time required for a successful search is $\Theta(2 + \alpha/2 - \alpha/2n) = \Theta(1 + \alpha)$.
\end{proof}

\vspace{2mm}
Thus, we see that if we take the length of the hash table to be at most proportional to the number of elements in the table, that is $n = O(m)$, we will have $\alpha = n/m = O(m)/m = O(1)$. And searching in the hash table takes constant time \underline{on average}. Moreover, all operations of the dictionary ADT can be implemented to take $O(1)$ time.


\subsubsection{Table Doubling}
So far we have not answered the question of ``how do we grow the hash table?''. That is, how do we pick a proper value for $m$, keeping in mind we want it to be $m = \Theta(n)$ (or $n = \Theta(n)$) so that $\alpha = \Theta(1)$. We give a short cover on the solution called \concept{table doubling}. A further study is done in section 17.4, about dynamic tables; which requires an amortized analysis, studied in depth in chapter 17.

The problem to study is when $m > n$, we need to grow the table: pick a new $m'$. Growing the table involves three steps.
\begin{smallenumerate}
    \item Allocate a new table $T'$, of size $m'$.
    \item Build a new hash function, $h'$.
    \item Rehash all items in the old table (for each element in $T$, $T'.insert(item)$).
\end{smallenumerate}
These three steps take together $\Theta(n + m + m') = \Theta(n)$ time. How much should we grow the table. Proposed solutions.
\begin{enumerate}[label=\roman*)]
    \item Take $m' = m + 1$. Then, the cost of $n$ insertions is $\Theta(1 + 2 + \dots + n) = \Theta(n^2)$.
    \item \textit{(Table doubling)} Take $m' = 2m$. Then, the cost of $n$ insertions is $\Theta(1 + 2 + 4 + 8 \,+ \stackrel{(\log{n})}{\dots} + \,n) = \Theta(n)$.
\end{enumerate}

We use amortized analysis to study the cost of table doubling: \textit{An operation takes ``$T(n)$ amortized'' if $k$ operations take $\leq k T(n)$ time}.

Hence, in table doubling, $k$ insertions take $\Theta(k)$ time. Thus, $\Theta(1)$ amortized time per operation.


Deletion needs a modification. It can be proven that defining table shrinking for if $m = n/4$ (this 4 could be any number $> 2$), then shrink table to $m/2$; gets amortized time of insert and delete operations to $\Theta(1)$ time.





\end{mysection}
