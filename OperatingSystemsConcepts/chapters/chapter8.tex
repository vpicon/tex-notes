\newpage 

\begin{multicols*}{2}
[\section{Main Memory}]


\subsection{Background}
    \textit{Memory} consists of a large array of bytes, each with its own address. The CPU fetches instructions from memory according to the value of the program counter. 

    The memory unit sees only a stream of memory addresses; it does not know how they are generated (by the instruction counter, indexing, indirection, literal addresses, and so on) or what they are for (instructions or data). Accordingly, we can ignore how a program generates a memory address. We are interested only in the sequence of memory addresses generated by the running program.

    \subsubsection{Basic Hardware.} Main memory and the registers built into the processor itself are the only general-purpose storage that the CPU can access directly. Therefore, any instructions in execution, and any data being used by the instructions, must be in one of these direct-access storage devices. If the data are not in memory, they must be moved there before the CPU can operate on them.
    

    For proper system operation we must protect the operating system from access by user processes, and processes from accessing other processes' memory. Since the \underline{operating system shouldn’t intervene between the CPU and} \underline{its memory accesses} (because of the resulting performance penalty), this protection is implemented at hardware level.

    First we need to make sure that each process has a separate memory space. We can provide this protection by using two registers: a \textit{base register}, which holds the smallest legal physical memory addres of the process; and the \textit{limit register}, which specifies the size of the range of memory available for the process (so that all memory addresses \texttt{n} that satisfy \texttt{base <= n < base + limit}, are available for a given process with such base and limit registers).

    Protection of memory space is accomplished by having the CPU hardware compare every address generated in user mode with the registers. Any attempt by a program executing in user mode to access operating-system memory or other users’ memory results in a trap to the operating system, which treats the attempt as a fatal error.   The base and limit registers can be loaded only by the operating system, which uses a special privileged instruction. Since privileged instructions can be executed only in kernel mode, and since only the operating system executes in kernel mode, only the operating system can load the base and limit registers.

\subsubsection{Address Binding.} A program residing on disk as a binary executable file, to be executed, must be brought into memory and placed within a process.
Depending on the memory management in use, the process may be moved
    between disk and memory during its execution. The processes on the disk that are waiting to be brought into memory for execution form the \textit{input queue}.

    Most systems allow a user process to reside in any part of the physical memory. Thus, although the address space of the computer may start at \texttt{0x00000}, the first address of the user process need not be \texttt{0x00000}.

    A user program goes through several steps before being executed. Addresses \underline{may be represented in different ways} during these steps.

    Addresses in the source program are generally symbolic. A compiler typically binds these symbolic addresses to relocatable addresses (such as “14 bytes from the beginning of this module”). The linkage editor or loader in turn \textit{binds} the relocatable addresses to absolute addresses (such as \texttt{0x74014}). Each binding is a \underline{mapping from one address space to another}. \\

    Binding of instructions and data to memory addresses can be done at any step along the way:
    \begin{itemize}
        \item \textbf{Compile time.} If you know at compile time where the process will reside in memory, then \textit{absolute code} can be generated. If at some time later, the starting locationchanges, it is necessary to recompile the code.
        \item \textbf{Load time.} If it is not known at compile time where the process will reside in memory, then the compiler must generate \textit{relocatable code}. In this case, final binding is delayed until load time (of the program into memory).
        \item \textbf{Execution time.} If the process can be moved during its execution from one memory segment to another, then binding must be delayed until run time. Most general-purpose operating systems use this method.
    \end{itemize}


    \subsubsection{Logical and Physical Address Space.} An address generated by the CPU is referred to as a \textit{logical address}, whereas an address seen by the memory unit (the address loaded into the memory-address register of the memory) is commonly referred to as a \textit{physical address}.
    The compile-time and load-time address-binding methods generate identical logical and physical addresses. The execution-time binding scheme results in differing logical and physical addresses. In this case, we usually refer to the logical address as a \textit{virtual address}.
    The set of all logical addresses generated by a program is a \textit{logical address space}. The set of all physical addresses corresponding to these logical addresses is a \textit{physical address space}. The run-time mapping from virtual to physical addresses is done by a hardware device called the \textit{memory-management unit} (MMU). 
    The user program deals with logical addresses and never deals with real physical addresses. The memory-mapping hardware converts logical addresses into physical addresses. The user program generates only virtual addresses and thinks that process runs in locations in the virtual address space, however these are maped to physical addresses before they are used.

\subsubsection{Dynamic Loading} So far, it has been necessary for the entire program and all data of a process to be in physical memory for the process to execute. The size of a process has thus been limited to the size of physical memory. To obtain better memory-space utilization, we can use dynamic loading: a routine is not loaded until it is called.


    \subsubsection{Dynamic Linking and Shared Libraries} \textit{Dynamically linked} libraries are system libraries that are linked to user
programs when the programs are run. Dynamic linking, is similar to dynamic loading. Here, though, linking, rather than loading, is postponed until execution time.

This feature is usually used with system libraries, such as language subroutine libraries. Without this facility, each program on a system must include a copy of its language library in the executable image. This requirement wastes both disk space and main memory.

    With dynamic linking, a \textit{stub} is included in the image for each library- routine reference. The stub is a small piece of code that indicates how to locate the appropriate memory-resident library routine or how to load the library if the routine is not already present. When the stub is executed, it checks to see whether the needed routine is already in memory. If it is not, the program loads the routine into memory. 


\subsection{Background}
    A process must be in memory to be executed. A process, however, can be \textit{swapped} temporarily out of memory to a backing store and then brought back into memory for continued execution.
    Swapping makes possible for the total physical address space of all processes to exceed the real physical memory of the system, thus increasing the degree of multiprogramming in a system.

    \subsubsection{Standard Swapping} It involves moving processes between main memory and a backing store. The backing store is commonly a fast disk, which mus be large enough to accommodate copies of all memory images for all users, and it must provide direct access to these memory images.
    The system maintains a \textit{ready queue} of all proesses whose memory images are on the backing store and ready to run. Whenever the CPU scheduler decides to execute a process, it calls the dispatcher. The dispatcher checks to see whether the next process in the queue is in memory. If it is not, and if there is no free memory region, the dispatcher swaps out a process currently in memory and swaps in the desired process. It then reloads registers and transfers control to the selected process. Context switch time is given by: $\Delta t = 2 \cdot size_{process}/r$, where $r$ is transfer rate of disk and 2 comes from swap in swap out.

    Clearly, it would be useful to know exactly how much memory a user process is using, not simply how much it might be using. Then we would need to swap only what is actually used, reducing swap time. For this method to be effective, the user must keep the system informed of any changes in memory requirements. Thus, a process with dynamic memory requirements will need to issue system calls (some \texttt{request\_memory()} and \texttt{release\_memory()}) to inform the operating system of its changing memory needs.

    If we want to swap a process, we must be sure that it is completely idle. Of particular concern is any pending I/O.

    Standard swapping is not used in modern operating systems. It requires too much swapping time and provides too little execution time. Modified versions of swapping, however, are
found on many systems.

\end{multicols*}
